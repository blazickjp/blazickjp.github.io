---
title: Dirichlet Processes
author: Joseph Blazick
date: '2021-04-03'
output:
  blogdown::html_page:
    highlight: tango
slug: []
categories: []
tags: []
Description: ''
Tags: []
Categories: []
DisableComments: no
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<div id="extending-to-infinite-components" class="section level2">
<h2>Extending to Infinite Components</h2>
<p>After working with mixture models, it’s natural to wonder how one should best determine the correct number of components to include in our models. One method for determining this is to extend our finite mixture model to the case of infinite components. Infinite components is going to require an infinite probability vector <span class="math inline">\(\pi\)</span> and typically when thinking about a Dirichlet Distribution, we have <span class="math inline">\(K &lt;&lt; N\)</span>. Now we might ask ourselves what happens when <span class="math inline">\(K&gt;&gt;N\)</span>? We know that the number of unique groups in our sample <span class="math inline">\(K^*\)</span> will be less than <span class="math inline">\(N\)</span>. To see what this may look like, we can generate <span class="math inline">\(\pi \sim Dir\left(\frac{\alpha}{K}, ...\right)\)</span> and then sample <span class="math inline">\(Z \sim cat(\pi)\)</span> with an arbitrarily large <span class="math inline">\(K\)</span> for increasing values of <span class="math inline">\(N\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> dirichlet, beta</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>K<span class="op">=</span><span class="dv">1000</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>alpha<span class="op">=</span><span class="dv">10</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>N<span class="op">=</span><span class="dv">500</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> num_clusters(alpha, K, N):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sorting the probability vector helps with the cluster naming</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> <span class="op">-</span>np.sort(<span class="op">-</span>dirichlet([alpha<span class="op">/</span>K]<span class="op">*</span>K).rvs().squeeze())</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.random.choice(K, N, p<span class="op">=</span>p, replace<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> [<span class="dv">1</span>]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(samples)):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> samples[i<span class="op">-</span><span class="dv">1</span>] <span class="kw">in</span> output:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            output.append(samples[i<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            output.append(np.<span class="bu">max</span>(output) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            np.where(samples <span class="op">==</span> i, i, samples)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> num_clusters(alpha<span class="op">=</span>alpha, K<span class="op">=</span>K, N<span class="op">=</span>N)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="bu">range</span>(<span class="bu">len</span>(y)), y, alpha <span class="op">=</span> <span class="fl">.6</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="bu">len</span>(y)), alpha<span class="op">*</span>np.log(<span class="dv">1</span> <span class="op">+</span> np.divide(<span class="bu">range</span>(<span class="bu">len</span>(y)), alpha)), color <span class="op">=</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">&quot;Number of Clusters (alpha = 10)&quot;</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;N&quot;</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Number Unique Clusters&quot;</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code></pre></div>
<p>It’s rather obvious to see that the number of clusters grows logarithmic in <span class="math inline">\(\alpha\)</span>, and we can actually plot the expected value in red as:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}\left[K^*\right|\alpha] &amp; = \alpha \log\left(1 + \frac{n}{\alpha}\right)
\end{align}
\]</span></p>
<p>So this helps us understand how the Dirichlet Distribution could work with large <span class="math inline">\(K\)</span> and givens us confidence that the distribution is well behaved as <span class="math inline">\(K\)</span> gets large, but what about when <span class="math inline">\(K\to\infty\)</span>?
### Stick Breaking</p>
<p>One common way to define the Dirichlet Process is by using the popular “stick breaking” construction. Formally, Let <span class="math inline">\((\phi_1,\phi_2, ...)\)</span> be a sequence of independent random variables distributed <span class="math inline">\(Beta(1, \alpha)\)</span>. Independent of this sequence
let <span class="math inline">\((Z_1, Z_2, ...)\)</span> be a sequence of random variables with base distribution <span class="math inline">\(H\)</span>, where above we used <span class="math inline">\(H \sim cat(\pi)\)</span>. If we define <span class="math inline">\(p_1 = \phi_1\)</span> and <span class="math inline">\(p_i = \phi_i\prod_{j&lt;i}(1-\phi_j)\)</span>, then the random measure <span class="math inline">\(P = \sum_{i\in \mathbb{N}}p_i\delta(Z_i)\)</span> is a Dirichlet Process with concentration parameter <span class="math inline">\(\alpha\)</span> and base measure <span class="math inline">\(H\)</span>. The nice thing about this formulation is that we can take finite samples from an infinite dimensional Dirichlet distribution by following this process.</p>
<p>To see this, we can generate samples using the stick-breaking method and verify that they are visually similar to those we generated using the Dirichlet Distribution for large values of <span class="math inline">\(K\)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stick_breaking(N, alpha):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    table <span class="op">=</span> [<span class="dv">1</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.random.beta(<span class="dv">1</span>, alpha)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    remaining_length <span class="op">=</span> <span class="dv">1</span><span class="op">-</span>beta</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    beta_vals <span class="op">=</span> [beta]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> [beta, <span class="dv">1</span><span class="op">-</span>beta]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, N):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(probs)), p<span class="op">=</span>np.array(probs)) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> sample <span class="op">==</span> <span class="bu">len</span>(probs):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            table.append(sample)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            new_beta <span class="op">=</span> np.random.beta(<span class="dv">1</span>, alpha)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            beta_vals.append(new_beta)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            probs[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> new_beta <span class="op">*</span> remaining_length</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            remaining_length <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(probs)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            probs.append(<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(probs))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:    </span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            table.append(sample)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> table, probs</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>y, prob <span class="op">=</span> stick_breaking(<span class="dv">500</span>, <span class="dv">10</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="bu">range</span>(<span class="bu">len</span>(y)), y, alpha <span class="op">=</span> <span class="fl">.6</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>ax.plot(<span class="bu">range</span>(<span class="bu">len</span>(y)), <span class="dv">10</span><span class="op">*</span>np.log(<span class="dv">1</span> <span class="op">+</span> np.divide(<span class="bu">range</span>(<span class="bu">len</span>(y)), <span class="dv">10</span>)), color <span class="op">=</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">&quot;Stick Breaking Process (alpha = 10)&quot;</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Iteration&quot;</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Class Assignment&quot;</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code></pre></div>
<div id="dirichlet-processes" class="section level3">
<h3>Dirichlet Processes</h3>
<p>A Dirichlet Process is best described as a stochastic process used in bayesian non-parametric modeling, and more specifically in Dirichlet Process Mixture Models (Infinite Mixture Models). Each draw from a DP is itself a distribution, making it a distribution over distributions. The DP derives it’s name from the fact that the marginal distributions of a DP is a finite dimensional Dirichlet Distribution just as a Gaussian Process has a finite dimensional Gaussian distributed marginal distribution. The DP has an infinite number of parameters which places it in the family of non-parametrics.</p>
</div>
</div>
