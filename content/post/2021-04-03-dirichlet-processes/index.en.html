---
title: Dirichlet Processes
author: Joseph Blazick
date: '2021-04-03'
slug: []
categories: []
tags: []
Description: ''
Tags: []
Categories: []
DisableComments: no
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<div id="extending-to-infinite-components" class="section level2">
<h2>Extending to Infinite Components</h2>
<p>After working with mixture models, it’s natural to wonder how one should best determine the correct number of components to include in our models. One method for determining this is to extend our finite mixture model to the case of infinite components. Infinite components is going to require an infinite probability vector <span class="math inline">\(\pi\)</span> and typically when thinking about a Dirichlet Distribution, we have <span class="math inline">\(K &lt;&lt; N\)</span>. Now we might ask ourselves what happens when <span class="math inline">\(K&gt;&gt;N\)</span>? We know that the number of unique groups in our sample <span class="math inline">\(K^*\)</span> will be less than <span class="math inline">\(N\)</span>. To see what this may look like, we can generate <span class="math inline">\(\pi \sim Dir\left(\frac{\alpha}{K}, ...\right)\)</span> and then sample <span class="math inline">\(Z \sim cat(\pi)\)</span> with an arbitrarily large <span class="math inline">\(K\)</span> for increasing values of <span class="math inline">\(N\)</span>.</p>
<pre class="python"><code>import numpy as np
import pandas as pd
import scipy as sp

from matplotlib import pyplot as plt
from scipy.stats import dirichlet, beta

K=1000
alpha=10
N=500

def num_clusters(alpha, K, N):
    # Sorting the probability vector helps with the cluster naming
    p = -np.sort(-dirichlet([alpha/K]*K).rvs().squeeze())
    samples = np.random.choice(K, N, p=p, replace=True) + 1
    output = [1]
    for i in range(1, len(samples)):
        if samples[i-1] in output:
            output.append(samples[i-1])
        else:
            output.append(np.max(output) + 1)
            np.where(samples == i, i, samples)
    return output

y = num_clusters(alpha=alpha, K=K, N=N)
fig, ax = plt.subplots()
ax.scatter(range(len(y)), y, alpha = .6)
ax.plot(range(len(y)), alpha*np.log(1 + np.divide(range(len(y)), alpha)), color = &quot;red&quot;)
fig.suptitle(&quot;Number of Clusters (alpha = 10)&quot;)
ax.grid()
plt.xlabel(&quot;N&quot;)
plt.ylabel(&quot;Number Unique Clusters&quot;)
fig.savefig(&quot;num_clusters.png&quot;)
plt.close(fig)</code></pre>
<p><img src="num_clusters.png" width="350" style="display: block; margin: auto;" /></p>
<p>It’s rather obvious to see that the number of clusters grows logarithmic in <span class="math inline">\(\alpha\)</span>, and we can actually plot the expected value in red as:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}\left[K^*\right|\alpha] &amp; = \alpha \log\left(1 + \frac{n}{\alpha}\right)
\end{align}
\]</span></p>
<p>So this helps us understand how the Dirichlet Distribution could work with large <span class="math inline">\(K\)</span> and givens us confidence that the distribution is well behaved as <span class="math inline">\(K\)</span> gets large, but what about when <span class="math inline">\(K\to\infty\)</span>? ### Stick Breaking</p>
<p>One common way to define the Dirichlet Process is by using the popular “stick breaking” construction. Formally, Let <span class="math inline">\((\phi_1,\phi_2, ...)\)</span> be a sequence of independent random variables distributed <span class="math inline">\(Beta(1, \alpha)\)</span>. Independent of this sequence let <span class="math inline">\((Z_1, Z_2, ...)\)</span> be a sequence of random variables with base distribution <span class="math inline">\(H\)</span>, where above we used <span class="math inline">\(H \sim cat(\pi)\)</span>. If we define <span class="math inline">\(p_1 = \phi_1\)</span> and <span class="math inline">\(p_i = \phi_i\prod_{j&lt;i}(1-\phi_j)\)</span>, then the random measure <span class="math inline">\(P = \sum_{i\in \mathbb{N}}p_i\delta(Z_i)\)</span> is a Dirichlet Process with concentration parameter <span class="math inline">\(\alpha\)</span> and base measure <span class="math inline">\(H\)</span>. The nice thing about this formulation is that we can take finite samples from an infinite dimensional Dirichlet distribution by following this process.</p>
<p>To see this, we can generate samples using the stick-breaking method and verify that they are visually similar to those we generated using the Dirichlet Distribution for large values of <span class="math inline">\(K\)</span>.</p>
<pre class="python"><code>def stick_breaking(N, alpha):
    table = [1]
    beta = np.random.beta(1, alpha)
    remaining_length = 1-beta
    beta_vals = [beta]
    probs = [beta, 1-beta]
    for i in range(1, N):
        sample = np.random.choice(range(len(probs)), p=np.array(probs)) + 1
        if sample == len(probs):
            table.append(sample)
            new_beta = np.random.beta(1, alpha)
            beta_vals.append(new_beta)
            probs[-1] = new_beta * remaining_length
            remaining_length = 1 - np.sum(probs)
            probs.append(1 - np.sum(probs))
        else:    
            table.append(sample)
    return table, probs

y, prob = stick_breaking(500, 10)
fig, ax = plt.subplots()
ax.scatter(range(len(y)), y, alpha = .6)
ax.plot(range(len(y)), 10*np.log(1 + np.divide(range(len(y)), 10)), color = &quot;red&quot;)
fig.suptitle(&quot;Stick Breaking Process (alpha = 10)&quot;)
ax.grid()
plt.xlabel(&quot;Iteration&quot;)
plt.ylabel(&quot;Class Assignment&quot;)
fig.savefig(&quot;stick_breaking.png&quot;)
plt.close(fig)</code></pre>
<p><img src="stick_breaking.png" width="350" style="display: block; margin: auto;" /></p>
<div id="dirichlet-processes" class="section level3">
<h3>Dirichlet Processes</h3>
<p>A Dirichlet Process is best described as a stochastic process used in bayesian non-parametric modeling, and more specifically in Dirichlet Process Mixture Models (Infinite Mixture Models). Each draw from a DP is itself a distribution, making it a distribution over distributions. The DP derives it’s name from the fact that the marginal distributions of a DP is a finite dimensional Dirichlet Distribution just as a Gaussian Process has a finite dimensional Gaussian distributed marginal distribution. The DP has an infinite number of parameters which places it in the family of non-parametric.</p>
</div>
</div>
