---
title: "Collapsed Gibbs Sampling and DPGMM"
author: "Josepg Blazick"
date: '2021-06-12'
output: pdf_document
categories: []
tags: []
Description: ''
Tags: []
Categories: []
DisableComments: no
slug: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this post I’m going to discuss collapsed gibbs sampling and how we can apply it to our Gaussian Mixture Model to model an Infinite Gaussian Mixture Model, also known as the Dirichlet Process Gaussian Mixture Model or DPGMM. The name is derived because we place a Dirichlet Process prior on the number of components allowing the number of components to be derived from the data instead of relying on the user to guess the correct number of components.</p>
</div>
<div id="collapsed-gibbs" class="section level1">
<h1>Collapsed Gibbs</h1>
<p>Collapsed gibbs sampling is similar to regular gibbs sampling except that we are going to integrate out <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(\pi\)</span> which allows us to reduce the number of sampling procedures required. It can also be shown that collapsed gibbs sampling will converge more efficiently in the MCMC chain. The reasons for this are beyond the scope of this post, but we might add another post later focusing on this topic. Looking back at our GMM from other posts, we know that we chose <span class="math inline">\(p(\pi|\alpha)\)</span> and <span class="math inline">\(p(\mu,\Sigma| \beta)\)</span> to be conjugate so we are actually able to integrate them out and only sample from <span class="math inline">\(p(z|z_{\neg i},\beta,\alpha)\)</span>. Here we’re using the notation <span class="math inline">\(z_{\neg i}\)</span> to denote the vector <span class="math inline">\(z\)</span> after removing <span class="math inline">\(z_i\)</span>. Kamper (2013) shows that:</p>
<p><span class="math display">\[
\begin{align*}
p(z_i=k|z_{\neg i},\mathcal{D},\beta,\alpha) &amp; \propto p(z_i=k|\alpha)p(x_i|\mathcal{D}_{\neg i},z_i = k,z_{\neg i}\beta)
\end{align*}
\]</span>
We can start by breakup up the right hand side of the statement and by focusing on the first term. Here we only need to integrate out <span class="math inline">\(\pi\)</span> from <span class="math inline">\(p(z_i=k|\alpha)\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
p(z_i = k|\alpha) &amp; = \int_{\pi}p(z|\pi)p(\pi|\alpha)d\pi\\
&amp; = \int_{\pi}\prod_{k=1}^K\pi_k^{N_k}\frac{1}{B(\alpha)}\prod_{k=1}^Kx_k^{\alpha_k -1}d\pi\\
&amp; = \frac{1}{B(\alpha)}\int_{\pi}\prod_{k=1}^K\pi_k^{N_k+\alpha_k -1}d\pi\\
\end{align*}
\]</span>
We can recognize that <span class="math inline">\(\pi_k^{N_k+\alpha_k -1}d\pi\)</span> as the un-normalized Dirichlet distribution. Taking advantage of this fact we can show that for any un-normalized probability distribution <span class="math inline">\(D\)</span>, <span class="math inline">\(\int D = \frac{1}{C}\)</span> where <span class="math inline">\(C\)</span> is the normalizing constant of the distribution. Therefore,</p>
<p><span class="math display">\[
\begin{align*}
p(z|\alpha) &amp; = \frac{\Gamma(\alpha)}{\Gamma(N+\alpha)}\prod_{k=1}^K\frac{\Gamma\left(N_K + \frac{\alpha}{k}\right)}{\Gamma\left(\frac{\alpha}{k}\right)}
\end{align*}
\]</span>
Now we can work to get this in the form that we want, which is as follows:
<span class="math display">\[
\begin{align*}
p(z_i =k | z_{\neg i}, \alpha) &amp; = \frac{p(z_i =k, z_{\neg i} | \alpha)}{p(z_{\neg i}| \alpha)}\\
&amp; = \frac{p(z_i = k | \alpha)}{p(z_{\neg i}| \alpha)}\\
&amp; = \frac{N_{k \neg i} + \frac{\alpha}{k}}{N+\alpha - 1}
\end{align*}
\]</span>
Next we can turn our attention to <span class="math inline">\(p(x_i|\mathcal{D}_{\neg i},z_i = k,z_{\neg i}\beta)\)</span> where <span class="math inline">\(\beta\)</span> represents the parameters to the Normal Inverse Wishart distribution. Using [1, p.843] we get:</p>
<p><span class="math display">\[
\begin{align*}
p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta) &amp; = p(x_i|\mathcal{D}_{k\neg i},\beta)\\
&amp; = \frac{p(x_i,\mathcal{D}_{k\neg i}|\beta)}{p(\mathcal{D}_{k\neg i}|\beta)}\\
&amp; = \frac{p(\mathcal{D}_{k}|\beta)}{p(\mathcal{D}_{k\neg i}|\beta)}
\end{align*}
\]</span></p>
<p>Which is equivalent to the posterior predictive distribution given by:</p>
<p><span class="math display">\[
\begin{align*}
p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta) &amp; = \mathcal{T}\left(x_i|\mu_o,\frac{\kappa_n+1}{\kappa_n(v_n - D+1)}S_n, v_n - D + 1\right)
\end{align*}
\]</span>
Now that we have everything that we need, we can formally write out the collapsed gibbs algorithm as follows:</p>
<head>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"
        integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous">
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>
</head>
<pre id="collapsed_gibbs" style="display:hidden;">
    \begin{algorithm}
    \caption{Collapsed Gibbs Sampler for a finite GMM}
    \begin{algorithmic}
    \STATE Choose an initial $\pmb{z}$
        \FOR{$T$ iterations}
          \FOR{$i=1$ to $N$}
            \STATE Remove $x_i$'s  statistics from $z_i$
            \FOR{$k=1$ to $K$}
            \STATE Calculate $p(z_i = k|z_{\neg i},\alpha)$
            \STATE Calculate $p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta)$
            \STATE Calculate $p(z_i = k |z_{\neg i},\mathcal{D},\alpha,\beta) \propto p(z_i = k|z_{\neg i},\alpha)p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta)$
            \ENDFOR
            \STATE Sample $k_{new}$ from $p(z_i = k |z_{\neg i},\mathcal{D},\alpha,\beta)$ after normalizing
            \STATE Add $x_i$'s statistic to the component $z_i = k_{new}$
          \ENDFOR
        \ENDFOR
    \end{algorithmic}
    \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("collapsed_gibbs"));
</script>
<p>Now let’s use the same data as from prior posts and solve the finite GMM case when <span class="math inline">\(k=3\)</span>.</p>
<pre class="python"><code>import numpy as np
from numpy.random import multinomial
from scipy.stats import dirichlet, multivariate_t, multivariate_normal
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import pandas as pd

# Define parameters for K=3 Mixture of Multivariate Gaussians
phi = [.3, .5, .2]
mu = np.array([[13,5], [0,-2], [-14,3]])
cov_1 = np.array([[2.0, 0.3], [0.3, 0.5]])
cov_2 = np.array([[3.0,.4], [.4,3.0]])    
cov_3 = np.array([[1.7,-.7], [-.7,1.7]])
cov = np.stack((cov_1, cov_2, cov_3), axis = 0)

def data_gen(mu, cov, phi, n):
    &quot;&quot;&quot;
    Generates samples from Mixture of K Multivariate Gaussian Distributions
    &quot;&quot;&quot;
    y = np.empty((n, 2))
    z = []
    for i in range(n):
        ind = multinomial(1, phi)
        for j, val in enumerate(ind):
            if val == 1:
                z.append(j)
                y[i,:] = np.random.multivariate_normal(mu[j,:], cov[j,:,:])
            else:
                next
    return np.array(y), z
  
data, z = data_gen(mu, cov, phi, 500)
x, y = np.mgrid[-17:17.5:.1, -7:7.5:.1]
pos = np.dstack((x,y))

fig, ax = plt.subplots()
ax.scatter(data[:,0], data[:,1], alpha = .6);
for i in range(3):
    ax.contour(x,y, multivariate_normal(mu[i,:], cov[i,:,:]).pdf(pos), extend=&#39;both&#39;);


fig.suptitle(&quot;K=3 GMM&quot;)
ax.grid()
fig.savefig(&quot;data_plot.png&quot;)
plt.close(fig)</code></pre>
<p><img src="data_plot.png" width="75%" height="75%" style="display: block; margin: auto;" /></p>
<p>And now we create our code for the collapsed gibbs sampler and fit it to our data.</p>
<pre class="python"><code>
def collapsed_gibbs(data, a, v, K, iters = 500):
    N = data.shape[0]
    D = data.shape[1]
    assert v &gt; D-1, &quot;v must be greater than D-1&quot;
    alpha = np.repeat(a, K)
    z = np.random.choice(K, size = N, replace = True, p =dirichlet(alpha / K).rvs().squeeze())
    for _ in range(iters):
        for i in range(N):
            # Remove x_i from data and Z
            d2 = np.delete(data, i, axis=0)
            z2 = np.delete(z, i, axis=0)
            p_z = []
            for k in range(K):
                mu_k = np.mean(d2[z2 == k], axis=0)
                # cov_k = np.cov(d2[z2 == k], rowvar=False)
                n_k = np.sum(z2 == k)
                p_z_k = (n_k + a/K) / (N + a - 1)
                S_k = np.dot(np.transpose(d2[z2 == k] - mu_k), d2[z2 == k] - mu_k) + np.eye(D)
                p_x_i = multivariate_t(mu_k, ((n_k+1) / (n_k *(n_k + v - D + 1)))*S_k, n_k+v - D + 1).pdf(data[i,])
                p_z_k = p_z_k * p_x_i
                p_z.append(p_z_k)
            # Standardize prob vector p(z_i = k)
            p_z = p_z / np.sum(p_z)
            z[i] = np.random.choice(K, 1, replace=True, p = p_z)

    return z

z_pred = collapsed_gibbs(data, 5, 2, K=3, iters=20)

fig, ax = plt.subplots()
cols = [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;]
for i, v in enumerate(cols):
    d2 = data[np.array(z_pred) == i,:]
    ax.scatter(d2[:,0], d2[:,1], color=v, alpha = .6);
    ax.contour(x,y, multivariate_normal(mu[i,:], cov[i,:,:]).pdf(pos), extend=&#39;both&#39;);


fig.suptitle(&quot;Collapsed Gibbs for Finite GMM&quot;)
ax.grid()
fig.savefig(&quot;collapsed_gibbs.png&quot;)
plt.close(fig)</code></pre>
<p><img src="collapsed_gibbs.png" width="75%" height="75%" style="display: block; margin: auto;" /></p>
<div id="extending-to-the-infinite-gmm" class="section level2">
<h2>Extending to the Infinite GMM</h2>
<p>Extending our existing model to the case with infinite clusters is rather straight forward from here. In my last post I discuss the Dirichlet Process and how you can model an infinite probability vector using the Chinese Restaurant Process.</p>
<p>Earlier we showed that,</p>
<p><span class="math display">\[
\begin{align*}
p(z_i =k | z_{\neg i}, \alpha) &amp; = \frac{N_{k \neg i} + \frac{\alpha}{k}}{N+\alpha - 1}
\end{align*}
\]</span>
In the case of infinite <span class="math inline">\(K\)</span> we end up with a CRP where:</p>
<p><span class="math display">\[
\begin{align*}
p(z_i =k | z_{\neg i}, \alpha) &amp; = \left\{
\begin{array}{ll}
      \frac{N_{k \neg i}}{N+\alpha - 1}  &amp; \text{if}\;\;\; N_k &gt; 0\\
      \frac{\alpha}{N+\alpha - 1}  &amp; \text{Otherwise}\\
\end{array} 
\right. \\
\end{align*}
\]</span>
Everything else is the same as the finite GMM giving the following Algorithm,</p>
<pre id="collapsed_gibbs_igmm" style="display:hidden;">
    \begin{algorithm}
    \caption{Collapsed Gibbs Sampler for an Infinite GMM}
    \begin{algorithmic}
    \STATE Choose an initial $\pmb{z}$
        \FOR{$T$ iterations}
          \FOR{$i=1$ to $N$}
            \STATE Remove $x_i$'s  statistics from $z_i$
            \FOR{$k=1$ to $K$}
              \STATE Calculate $p(z_i = k|z_{\neg i},\alpha)$
              \STATE Calculate $p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta)$
              \STATE Calculate $p(z_i = k |z_{\neg i},\mathcal{D},\alpha,\beta) \propto p(z_i = k|z_{\neg i},\alpha)p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta)$
            \ENDFOR
            \STATE Calculate $p(z_i =k^* | z_{\neg i}, \alpha) = \frac{\alpha}{N+\alpha - 1}$
            \STATE Calculate $p(x_i|\beta)$ Under the Prior
            \STATE Calculate $p(z_i = k^* |z_{\neg i},\mathcal{D},\alpha,\beta) \propto p(z_i =k^* | z_{\neg i}, \alpha)p(x_i|\beta)$
            \STATE Sample $k_{new}$ from $p(z_i = k |z_{\neg i},\mathcal{D},\alpha,\beta)$ after normalizing
            \STATE Add $x_i$'s statistic to the component $z_i = k_{new}$
            \STATE If any component is empty, remove it and decrease $K$
          \ENDFOR
        \ENDFOR
    \end{algorithmic}
    \end{algorithm}
</pre>
<script>
    pseudocode.renderElement(document.getElementById("collapsed_gibbs_igmm"));
</script>
<p>Now we can create the code for our Infinite GMM Gibbs Sampler.</p>
<pre class="python"><code>def collapsed_gibbs_IGMM(data, a, v, K, iters = 500):
    N = data.shape[0]
    D = data.shape[1]
    assert v &gt; D-1, &quot;v must be greater than D-1&quot;
    alpha = np.repeat(a, K)
    z = np.random.choice(K, size = N, replace = True, p =dirichlet(alpha / K).rvs().squeeze())
    df = pd.DataFrame({&quot;k&quot;: z, &quot;p_z&quot;: np.nan})
    g_not = []
    for q in range(iters):
        for i in range(N):
            groups = list(np.unique(z))
            # Remove x_i from data and Z
            d2 = np.delete(data, i, axis=0)
            z2 = np.delete(z, i, axis=0)
            p_z = []
            g = []
            for k in groups:
                if np.sum([z2 == k]) == 0:
                    g_not.append(k)
                    # This is the last group so just move on
                    continue
                
                mu_k = np.mean(d2[z2 == k], axis=0)
                # cov_k = np.cov(d2[z2 == k], rowvar=False)
                n_k = np.sum(z2 == k)
                p_z_k = (n_k + a/K) / (N + a - 1)
                S_k = np.dot(np.transpose(d2[z2 == k] - mu_k), d2[z2 == k] - mu_k) + np.eye(D)
                p_x_i = multivariate_t(mu_k, ((n_k+1) / (n_k *(n_k + v - D + 1)))*S_k, n_k+v - D + 1).pdf(data[i,])
                p_z_k = p_z_k * p_x_i
                g.append(k)
                p_z.append(p_z_k)
            
            # Now consider new component
            p_z_k = (a / len(groups)) / (N + (a/len(groups)) - 1)
            p_x_i = multivariate_t([0 ,0], np.eye(D), v - D + 1).pdf(data[i,])
            p_z_k = p_z_k * p_x_i
            g.append(k+1)
            p_z.append(p_z_k)
            # Standardize prob vector p(z_i = k)
            p_z = p_z / np.sum(p_z)
            z[i] = np.random.choice(g, 1, replace=True, p = p_z)
        df = pd.DataFrame({
            &#39;z&#39;: z,
        }).groupby([&#39;z&#39;]).size().reset_index(name=&#39;counts&#39;)
        # Rename clusters to reindex to zero
        # z and z_new are the same thing, but z_new has cluster names from 0 - K
        # where z has cluster names &gt; K. This can be a problem when using the cluster
        # name to subset a list of colors (which is why we rename them)
        lookup = df.to_dict()[&#39;z&#39;]
        lookup = dict((v,k) for k, v in lookup.items()) 
        z_new = np.array([lookup.get(i) for i in z])
    return z, df, z_new

z_pred, df, z_new = collapsed_gibbs_IGMM(data, 1, 10, K=5, iters=50)

fig, ax = plt.subplots()
# Given that we expect 3 clusters, we assume we won&#39;t need more than 6 colors
cols = [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;grey&quot;, &quot;yellow&quot;, &quot;black&quot;]
# Filter down to required colors. 
cols1 = [cols[i] for i, v in enumerate(np.unique(z_new))]
print(cols)</code></pre>
<pre><code>## [&#39;red&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;grey&#39;, &#39;yellow&#39;, &#39;black&#39;]</code></pre>
<pre class="python"><code>for i, v in enumerate(cols1):
    d2 = data[np.array(z_new) == i,:]
    ax.scatter(d2[:,0], d2[:,1], color=v, alpha = .6, label = v)
    if i in range(3):
        ax.contour(x,y, multivariate_normal(mu[i,:], cov[i,:,:]).pdf(pos), extend=&#39;both&#39;)


fig.suptitle(f&quot;Collapsed Gibbs for Infinite GMM, {df.shape[0]} Clusters Found&quot;)
ax.grid()
fig.legend()
fig.savefig(&quot;collapsed_gibbs_igmm.png&quot;)
plt.close(fig)</code></pre>
<p><img src="collapsed_gibbs_igmm.png" width="75%" height="75%" style="display: block; margin: auto;" /></p>
<p>Note that we do have a few extra clusters here, but for the most part our output looks pretty good. With our gibbs sampler, we will continue to add and remove clusters but we’ve clearly converged to approximately 3 total clusters.</p>
</div>
</div>
