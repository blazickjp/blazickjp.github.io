<!DOCTYPE html>
<html lang="en" data-theme=""><head>
    <title> Joseph Blazick | Collapsed Gibbs Sampling and DPGMM </title>

    
    <meta charset="utf-8"><meta name="generator" content="Hugo 0.82.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
    <meta name="description" content="Data Science and Optimization Manager at Lucas Systems Inc.">
    

    <link rel="stylesheet" 
          href="/css/github-gist.min.705a0defad561099ccdda10f51afc4119b3c94e8d0d9edd6308d8a4b1b552444.css" 
          type="text/css">
    
    
    <link rel="stylesheet"
          href="/css/style.min.2277e4d1f5f913138c1883033695f7a9779a2dcdc66ae94d514bd151bebd8f78.css"
          integrity="sha256-Infk0fX5ExOMGIMDNpX3qXeaLc3GaulNUUvRUb69j3g="
          crossorigin="anonymous"
          type="text/css">
    
    <link rel="stylesheet"
        href="/css/markupHighlight.min.f798cbda9aaa38f89eb38be6414bd082cfd71a6780375cbf67b6d2fb2b96491e.css"
        integrity="sha256-95jL2pqqOPies4vmQUvQgs/XGmeAN1y/Z7bS&#43;yuWSR4="
        crossorigin="anonymous"
        type="text/css">
    
    <link rel="stylesheet" 
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" 
    integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" 
    crossorigin="anonymous" />

    
    <link rel="shortcut icon" href="/favicons/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">

    <link rel="canonical" href="/post/2021-06-12-collapsed-gibbs-sampling/">

    
    
    

    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
    
    <script type="text/javascript"
            src="/js/anatole-header.min.0c05c0a90d28c968a1cad4fb31abd0b8e1264e788ccefed022ae1d3b6f627514.js"
            integrity="sha256-DAXAqQ0oyWihytT7MavQuOEmTniMzv7QIq4dO29idRQ="
            crossorigin="anonymous"></script>


    
        
        
        <script type="text/javascript"
                src="/js/anatole-theme-switcher.min.8ef71e0fd43f21a303733dbbecae5438b791d2b826c68a9883bd7aa459a076d2.js"
                integrity="sha256-jvceD9Q/IaMDcz277K5UOLeR0rgmxoqYg716pFmgdtI="
                crossorigin="anonymous"></script>
    
    <meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://blazickjp.github.io/105"/>

<meta name="twitter:title" content="Collapsed Gibbs Sampling and DPGMM"/>
<meta name="twitter:description" content="Introduction In this post I’m going to discuss collapsed gibbs sampling and how we can apply it to our gaussian mixture model to model an Infinite Gaussian Mixture Model, aslo known as the Dirichlet Process Gaussian Mixture Model or DPGMM. The name is derived because we place a Dirichlet Process prior on the number of components allowing the number of components to be derived from the data instead of relying on the user to guess the correct number of components.
 Collapsed Gibbs Collapsed gibbs sampling is similar to regular gibbs sampling except that we are going to integrate out some parameters which allows us to reduce the number of sampling procedures required."/>


    
</head>
<body><div class="sidebar . ">
    <div class="logo-title">
        <div class="title">
            <img src="/images/joe2.jpg" alt="profile picture">
            <h3 title=""><a href="/">Joseph Blazick</a></h3>
            <div class="description">
                <p>Data Science and Optimization Manager at Lucas Systems Inc.</p>
            </div>
        </div>
    </div>
    <ul class="social-links">
        
            <li>
                <a href="https://www.linkedin.com/in/joe-blazick-119307a2/" rel="me" aria-label="Linkedin">
                    <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
                </a>
            </li>
        
            <li>
                <a href="https://github.com/blazickjp" rel="me" aria-label="GitHub">
                    <i class="fab fa-github fa-2x" aria-hidden="true"></i>
                </a>
            </li>
        
            <li>
                <a href="mailto:joe.blazick@yahoo.com" rel="me" aria-label="e-mail">
                    <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
                </a>
            </li>
        
    </ul>
    <div class="footer">
        <div class="by_farbox">&copy; Joseph Blazick  2021 </div>
    </div>
</div>
<div class="main">
    <div class="page-top  . ">
    <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
    </a>
    <ul class="nav" id="navMenu">
        
        
            
            <li><a 
                   href="/"
                        
                   title="">Home</a></li>
        
            
            <li><a 
                   href="/post/"
                        
                   title="">Posts</a></li>
        
            
            <li><a 
                   href="/about/"
                        
                   title="">About</a></li>
        
            
            <li><a 
                   href="/contact/"
                        
                   title="">Contact</a></li>
        
        
        
            <li class="theme-switch-item">
                <a class="theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a>
            </li>
        
    </ul>
</div>

    <div class="autopagerize_page_element">
        <div class="content">
    <div class="post  . ">
        <div class="post-content">
            
            <div class="post-title">
                <h3>Collapsed Gibbs Sampling and DPGMM</h3>
                
                    <div class="info">
                        <em class="fas fa-calendar-day"></em>
                        <span class="date"> Sat, Jun 12, 2021 
                                           </span>
                        <em class="fas fa-stopwatch"></em>
                        <span class="reading-time">4-minute read</span>
                    </div>
                
            </div>

            
<script src="http://blazickjp.github.io/post/2021-06-12-collapsed-gibbs-sampling/index.en_files/header-attrs/header-attrs.js"></script>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this post I’m going to discuss collapsed gibbs sampling and how we can apply it to our gaussian mixture model to model an Infinite Gaussian Mixture Model, aslo known as the Dirichlet Process Gaussian Mixture Model or DPGMM. The name is derived because we place a Dirichlet Process prior on the number of components allowing the number of components to be derived from the data instead of relying on the user to guess the correct number of components.</p>
</div>
<div id="collapsed-gibbs" class="section level1">
<h1>Collapsed Gibbs</h1>
<p>Collapsed gibbs sampling is similar to regular gibbs sampling except that we are going to integrate out some parameters which allows us to reduce the number of sampling procedures required. Looking back at our GMM from other posts, we know that we chose <span class="math inline">\(p(\pi|\alpha)\)</span> and <span class="math inline">\(p(\mu,\Sigma| \beta)\)</span> to be conjugate so we are actually able to integrate them out and only sample from <span class="math inline">\(p(z|z_{\neg i},\beta,\alpha)\)</span>. Kamper (2013) shows that:
<span class="math display">\[
\begin{align*}
p(z_i=k|z_{\neg i},\mathcal{D},\beta,\alpha) &amp; \propto p(z_i=k|\alpha)p(x_i|\mathcal{D}_{\neg i},z_i = k,z_{\neg i}\beta)
\end{align*}
\]</span>
Focusing on the first term we can start by integrating out <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
p(z|\alpha) &amp; = \int_{\pi}p(z|\pi)p(\pi|\alpha)d\pi\\
&amp; = \int_{\pi}\prod_{k=1}^K\pi_k^{N_k}\frac{1}{B(\alpha)}\prod_{k=1}^Kx_k^{\alpha_k -1}d\pi\\
&amp; = \frac{1}{B(\alpha)}\int_{\pi}\prod_{k=1}^K\pi_k^{N_k+\alpha_k -1}d\pi\\
\end{align*}
\]</span>
Here we recognize that the integral of the 2nd term is the inverse of the normalizing constand from the Dirichlet distribution. This trick is why we are allowed to integrate out these parameters and we will reuse this trick when dealing with <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>. Therefore,</p>
<p><span class="math display">\[
\begin{align*}
p(z|\alpha) &amp; = \frac{\Gamma(\alpha)}{\Gamma(N+\alpha)}\prod_{k=1}^K\frac{\Gamma\left(N_K + \frac{\alpha}{k}\right)}{\Gamma\left(\frac{\alpha}{k}\right)}
\end{align*}
\]</span>
Now we can work to get this in the form that we want, which is as follows:
<span class="math display">\[
\begin{align*}
p(z_i =k | z_{\neg i}, \alpha) &amp; = \frac{p(z_i =k, z_{\neg i} | \alpha)}{p(z_{\neg i}| \alpha)}\\
&amp; = \frac{p(z_i = k | \alpha)}{p(z_{\neg i}| \alpha)}\\
&amp; = \frac{N_{k \neg i} + \frac{\alpha}{k}}{N+\alpha - 1}
\end{align*}
\]</span>
Next we can turn our attention to <span class="math inline">\(p(x_i|\mathcal{D}_{\neg i},z_i = k,z_{\neg i}\beta)\)</span> where <span class="math inline">\(\beta\)</span> represents the parameters to the Normal Inverse Wishart distribution. Using [1, p.843] we get:</p>
<p><span class="math display">\[
\begin{align*}
p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta) &amp; = p(x_i|\mathcal{D}_{k\neg i},\beta)\\
&amp; = \frac{p(x_i,\mathcal{D}_{k\neg i}|\beta)}{p(\mathcal{D}_{k\neg i}|\beta)}\\
&amp; = \frac{p(\mathcal{D}_{k}|\beta)}{p(\mathcal{D}_{k\neg i}|\beta)}
\end{align*}
\]</span></p>
<p>Which is equivalent to the posterior predictive distribution given by:</p>
<p><span class="math display">\[
\begin{align*}
p(x_i|\mathcal{D}_{k\neg i},z_i = k,z_{\neg i}\beta) &amp; = \mathcal{T}\left(x_i|\mu_o,\frac{\kappa_n+1}{\kappa_n(v_n - D+1)}S_n, v_n - D + 1\right)
\end{align*}
\]</span>
Now let’s use the same data as from prior posts and solve the finite GMM case when <span class="math inline">\(k=3\)</span>.</p>
<pre class="python"><code>import numpy as np
from numpy.random import multinomial
from scipy.stats import dirichlet, multivariate_t, multivariate_normal
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import pandas as pd

# Define parameters for K=3 Mixture of Multivariate Gaussians
phi = [.3, .5, .2]
mu = np.array([[13,5], [0,-2], [-14,3]])
cov_1 = np.array([[2.0, 0.3], [0.3, 0.5]])
cov_2 = np.array([[3.0,.4], [.4,3.0]])    
cov_3 = np.array([[1.7,-.7], [-.7,1.7]])
cov = np.stack((cov_1, cov_2, cov_3), axis = 0)

def data_gen(mu, cov, phi, n):
    &quot;&quot;&quot;
    Generates samples from Mixture of K Multivariate Gaussian Distributions
    &quot;&quot;&quot;
    y = np.empty((n, 2))
    z = []
    for i in range(n):
        ind = multinomial(1, phi)
        for j, val in enumerate(ind):
            if val == 1:
                z.append(j)
                y[i,:] = np.random.multivariate_normal(mu[j,:], cov[j,:,:])
            else:
                next
    return np.array(y), z
  
data, z = data_gen(mu, cov, phi, 500)
x, y = np.mgrid[-17:17.5:.1, -7:7.5:.1]
pos = np.dstack((x,y))

fig, ax = plt.subplots()
ax.scatter(data[:,0], data[:,1], alpha = .6)
for i in range(3):
    ax.contour(x,y, multivariate_normal(mu[i,:], cov[i,:,:]).pdf(pos), extend=&#39;both&#39;)
</code></pre>
<pre><code>## &lt;matplotlib.contour.QuadContourSet object at 0x7fedf6135cd0&gt;
## &lt;matplotlib.contour.QuadContourSet object at 0x7fedf615dca0&gt;
## &lt;matplotlib.contour.QuadContourSet object at 0x7fedf5ff91f0&gt;</code></pre>
<pre class="python"><code>fig.suptitle(&quot;K=3 GMM&quot;)
ax.grid()
fig.savefig(&quot;data_plot.png&quot;)
plt.close(fig)</code></pre>
<p><img src="data_plot.png" width="75%" height="75%" style="display: block; margin: auto;" /></p>
<p>And now we create our code for the collapsed gibbs sampler and fit it to our data.</p>
<pre class="python"><code>
def collapsed_gibbs(data, a, v, K, iters = 500):
    N = data.shape[0]
    D = data.shape[1]
    assert v &gt; D-1, &quot;v must be greater than D-1&quot;
    alpha = np.repeat(a, K)
    z = np.random.choice(K, size = N, replace = True, p =dirichlet(alpha / K).rvs().squeeze())
    for _ in range(iters):
        for i in range(N):
            # Remove x_i from data and Z
            d2 = np.delete(data, i, axis=0)
            z2 = np.delete(z, i, axis=0)
            p_z = []
            for k in range(K):
                mu_k = np.mean(d2[z2 == k], axis=0)
                # cov_k = np.cov(d2[z2 == k], rowvar=False)
                n_k = np.sum(z2 == k)
                p_z_k = (n_k + a/K) / (N + a - 1)
                S_k = np.dot(np.transpose(d2[z2 == k] - mu_k), d2[z2 == k] - mu_k) + np.eye(D)
                p_x_i = multivariate_t(mu_k, ((n_k+1) / (n_k *(n_k + v - D + 1)))*S_k, n_k+v - D + 1).pdf(data[i,])
                p_z_k = p_z_k * p_x_i
                p_z.append(p_z_k)
            # Standardize prob vector p(z_i = k)
            p_z = p_z / np.sum(p_z)
            z[i] = np.random.choice(K, 1, replace=True, p = p_z)

    return z

z_pred = collapsed_gibbs(data, 5, 2, K=3, iters=20)

fig, ax = plt.subplots()
cols = [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;]
for i, v in enumerate(cols):
    d2 = data[np.array(z_pred) == i,:]
    ax.scatter(d2[:,0], d2[:,1], color=v, alpha = .6)
    ax.contour(x,y, multivariate_normal(mu[i,:], cov[i,:,:]).pdf(pos), extend=&#39;both&#39;)
</code></pre>
<pre><code>## &lt;matplotlib.collections.PathCollection object at 0x7fedf78ba310&gt;
## &lt;matplotlib.contour.QuadContourSet object at 0x7fedf78bad60&gt;
## &lt;matplotlib.collections.PathCollection object at 0x7fedf78babb0&gt;
## &lt;matplotlib.contour.QuadContourSet object at 0x7fedf78bac70&gt;
## &lt;matplotlib.collections.PathCollection object at 0x7fedf78c8af0&gt;
## &lt;matplotlib.contour.QuadContourSet object at 0x7fedf78ba2b0&gt;</code></pre>
<pre class="python"><code>fig.suptitle(&quot;Collapsed Gibbs for Finite GMM&quot;)
ax.grid()
fig.savefig(&quot;collapsed_gibbs.png&quot;)
plt.close(fig)</code></pre>
<p><img src="collapsed_gibbs.png" width="75%" height="75%" style="display: block; margin: auto;" /></p>
</div>
</div>
        <div class="post-footer">
            <div class="info">
                
                
            </div>
        </div>

        
    </div>


        </div>
    </div>
</div>

<script type="text/javascript"
        src="/js/medium-zoom.min.2d6fd0be87fa98f0c9b4dc2536b312bbca48757f584f6ea1f394abc9bcc38fbc.js"
        integrity="sha256-LW/Qvof6mPDJtNwlNrMSu8pIdX9YT26h85SrybzDj7w="
        crossorigin="anonymous"></script><link rel="stylesheet"
              href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css"
              integrity="sha384-t5CR&#43;zwDAROtph0PXGte6ia8heboACF9R5l/DiY&#43;WZ3P2lxNgvJkQk5n7GPvLMYw"
              crossorigin="anonymous"><script defer
                src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js"
                integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8&#43;w2LAIftJEULZABrF9PPFv&#43;tVkH"
                crossorigin="anonymous"></script><script defer
                src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js"
                integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB&#43;w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v"
                crossorigin="anonymous"
                onload="renderMathInElement(document.body);"></script></body>

</html>
